@article{Zhao2022,
  archiveprefix   = {arXiv},
  arxivid         = {2009.00155},
  author          = {Zhao, Sicheng and Yue, Xiangyu and Zhang, Shanghang and Li, Bo and Zhao, Han and Wu, Bichen and Krishna, Ravi and Gonzalez, Joseph E. and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A. and Keutzer, Kurt},
  doi             = {10.1109/TNNLS.2020.3028503},
  eprint          = {2009.00155},
  file            = {:Users/bdsaglam/Documents/Mendeley Desktop/Zhao et al. - 2022 - A Review of Single-Source Deep Unsupervised Visual Domain Adaptation.pdf:pdf},
  issn            = {21622388},
  journal         = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords        = {Adversarial learning,discrepancy-based methods,domain adaptation (DA),self-supervised learning (SSL),transfer learning},
  mendeley-groups = {Thesis/reviews},
  month           = {feb},
  number          = {2},
  pages           = {473--493},
  pmid            = {33095718},
  publisher       = {Institute of Electrical and Electronics Engineers Inc.},
  title           = {{A Review of Single-Source Deep Unsupervised Visual Domain Adaptation}},
  volume          = {33},
  year            = {2022}
}

@article{Csurka2017,
  abstract        = {The aim of this paper is to give an overview of domain adaptation and
                     transfer learning with a specific view on visual applications. After a general
                     motivation, we first position domain adaptation in the larger transfer learning
                     problem. Second, we try to address and analyze briefly the state-of-the-art
                     methods for different types of scenarios, first describing the historical
                     shallow methods, addressing both the homogeneous and the heterogeneous domain
                     adaptation methods. Third, we discuss the effect of the success of deep
                     convolutional architectures which led to new type of domain adaptation methods
                     that integrate the adaptation within the deep architecture. Fourth, we overview
                     the methods that go beyond image categorization, such as object detection or
                     image segmentation, video analyses or learning visual attributes. Finally, we
                     conclude the paper with a section where we relate domain adaptation to other
                     machine learning solutions.},
  archiveprefix   = {arXiv},
  arxivid         = {1702.05374},
  author          = {Csurka, Gabriela},
  doi             = {10.48550/arxiv.1702.05374},
  eprint          = {1702.05374},
  file            = {:Users/bdsaglam/Documents/Mendeley Desktop/Csurka - 2017 - Domain Adaptation for Visual Applications A Comprehensive Survey.pdf:pdf},
  mendeley-groups = {Thesis/reviews},
  month           = {feb},
  title           = {{Domain Adaptation for Visual Applications: A Comprehensive Survey}},
  url             = {https://arxiv.org/abs/1702.05374v2},
  year            = {2017}
}

@article{kouw2019review,
  title={A review of domain adaptation without target labels},
  author={Kouw, Wouter M and Loog, Marco},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={3},
  pages={766--785},
  year={2019},
  publisher={IEEE}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{dalle,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@inproceedings{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
